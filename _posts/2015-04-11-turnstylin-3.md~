---
layout: post
title: "Turnstylin' and Profilin' III: Analysis"
date: 2015-04-12 00:00:00
categories: [Metis, MTA]
tags: Metis, MTA
excerpt: "Results, comparisons, and caveats for outreach strategies based on MTA data."
---

#### Results

In the previous post, I showed how I processed and analyzed the MTA data to create recommendations for the organization to deploy street teams to stations near universities. There are still several other demographics, as outlined in the general strategy! (These results were generated by other members of this project team.)

##### Total Volume

If we want to maximize exposure, we're interested in traffic as a whole, without filtering for demographics. To this end, we ranked all stations by average hourly traffic (again, between mid-April and the end of May).

![averagehourly](assets/allaveragehourly.jpg)

Unsurprisingly, the top stations are major commuter hubs: GCT, 34th St, Times Square, Penn Station, and Union Square. 

I think this data would have been better presented as daily totals. It's the same number, but I think using hourly average kind of masks the fact that there are strong time-of-day trends. This doesn't really matter as far as the interpretation goes, because we're interested in all traffic at all times, but an hourly average suggests a constancy of flow that isn't really the case. 

Daily totals would also be a better match for the other pieces of data analysis, which use station-day as the unit of analysis. More temporal splitting would also allow us to find the highest traffic periods within the day, which would further help specify best street team deployments. This would require a lot more cleaning than we were able to do, though. I was lucky in that my set of university stations didn't have the time-coding issues I mentioned above, but there is significant inconsistency both among and within the whole set of stations.

##### Tech findings

In the last post, I described how I identified the set of stations that might see usage by a higher proportion of people who work in the tech sector. Here's a map of the areas and a ranked list of stations falling within them:

![tech](assets/tech.jpg)

In general, there's a lot more traffic in Manhattan, as expected. 

This data is given as average daily total, which I think should be further restricted to weekday traffic (though I don't think the overall ranking or magnitudes of usage volume would change). Of course, we can do the same analysis as with the universities and further choose best weekday/time combinations for each station.

##### Income findings

Targeting high-income residents again requires first selecting stations based on a given criterion (in this case, location in one of the top-ten highest median income census tracts) and then comparing traffic rates between those stations. Here's the list:

![highincome](assets/highincome.jpg)

This data is presented as average AM entries and PM exits in order to capture residents. Interestingly, station-time usage figures do not always covary: having the highest usage in the morning does not imply having highest usage at night. 

This breakdown also suggests the use of an interaction variable, median income * entry/exit rate, to capture the amount of money flowing through the station. This would allow us to compare, for example, the relative benefit of choosing a somewhat lower median-income station with higher traffic over the highest-income station, which may have lower traffic (e.g. you might choose Union Square at night rather than Chambers St in the morning). 

#### Conclusions

##### Overlap

![alltopstations](assets/all.jpg)

As you can see, the top five stations in each category are quite spatially distributed. I was surprised by this finding, as I assumed that they would likely dovetail, but upon further reflection, there are socioeconomic reasons why the richest neighborhoods aren't the ones closest to high-traffic transportation hubs, etc. City planning may account for some of the distribution of universities, and it may be that now-booming tech sectors were originally placed in areas away from major stations with lower real estate costs.

![overlap](assets/overlap.jpg)

This table shows that there are a few stations that might do double-duty, but unfortunately, there's no single station that is going to hit all demographics. This is a given between the university and tech stations, as they don't have a single selection in common. As far as incorporating total traffic and grabbing high-income residents at the same time as one of the interest demographics, the Columbus Circle station looks like a good compromise, but it's hardly a clear-cut winner.

##### Recommendations

So, given these findings, what do we propose to this organization? 

Generally, for this gala, I would suggest focusing on the demographically targeted stations, since finding donors and attendees is the primary goal.

However, I don't have any specific recommendations (I know it's kind of a copout) because ultimately it depends on the resources and priorities of the organization at a given time. You can imagine them saying, for example, for this campaign, we only have three street teams available per day, and it's most important for us to find young women in tech who will become the next generation of supporters. In that case we might choose more university stations despite comparatively lower traffic. The point is that the mix is flexible. For a publicity push, the organization might instead be best served placing teams at the top high-volume stations. As the organization increases its street team availability, the set of stations can expand.

The data strategy used in this project is really more about creating a set of options - a tool - that can flexibly accomodate an organization's needs. One further step might be to define a metric that allows comparisons across stations (traffic * conversion rate * income, for example?) and then build a model that lets organizations assign a multiplier that represents its priorities. 

More simply, an organization might say, we want to devote 60% of our outreach activities to well-heeled donors, 20% to tech nerds, and 20% to visibility. Then we would simply assign the available resources to each category, moving down the rankings to capture the best station/day/time in each. (In fact, we can even drill down to best station _entrance_/day/time.)

##### Implementation

This is the most intriguing and yet challenging part of the project to me, as a recovering academic. We have an effectiveness model undergirded with some assumptions and we try to maximize based on those givens. This can lead to some fairly subtle - even specious? - differences that affect ranking but may not actually be signifiant in real life.  Is there really a difference between residents living in the 232k median income area near the Chambers St station and residents of the 220k median income area near Franklin St? (These stations are one stop apart, by the way, and yet, depending on the cutoff we use, we may choose Chambers and several others before we get to Franklin.)

More seriously, the limits of implementation mean that to some extent, this model may not even matter. It could be enough to simply go on theoretical intuition without using the data! I'm reminded of something an architect once told me: she said she had learned a number of complicated, highly precise equations for calculating load-bearing capacity of a building, but in the end, to guarantee safety, the result was simply multiplied by two! Likewise, these targeting approaches cut fairly finely, but any distinctions in the data are overridden by the actual process of canvassing in real life.

Consider that each member of a street team can only talk to a certain number of people a day. The traffic volume differences between stations near NYU and the City College station may not even matter, since above a fairly low threshold, most of the traffic is extraneous - you won't be able to talk to them anyway. 

This issue is partially solved by the inclusion of the exposure goal: all things equal, it is then better to have a presence at a busy university station because even though you might collect the same 100 signatures from college women studying computer science (or whatever), you have the additional benefit of 60k more sets of eyeballs seeing your logo and being reminded of your cause every day.

##### Model-fying

The good thing is that this approach is flexible and iterative. Once we do one round of outreach, we can learn from its results and do better targeting. For example, we might compare the emails collected at university stations to non-university stations. Is there a higher percentage of .edu domains? If so, we might be more sure that we are capturing the population of interest. Extrapolating further, we can use that rate as a rough estimate of targetting success and build that into the model (e.g. choosing a tech/university station gets you a 30% increase in likelihood of capturing a member of the relevant demographics).

More generally, these success rates will inform future strategies, as we can match actual donors/participants with where they were approached. 

Alternatively, we can imagine a campaign in which the organization wants to collect 50k signatures in a month. Once we have some information on conversion rates (aka average signatures per 10k riders), we will be able to efficiently allocate street team resources. 50k signatures might mean five teams at one station with a 200k rider daily average, or it might mean one team each at five stations where the daily average ridership sums to 200k total(or a combination approach, taking into account any target populations).

We don't even necessarily have to collect all this data ourselves: there's a lot of both experimental and experiential data from other nonprofits engaged in similar fundraising/awareness-building efforts that can be used to estimate conversion rates, for example. It might even be useful to simply get qualitative recommendations - perhaps the layout of one station is more conducive to waylaying commuters than others.

Further, the importance of the statistical likelihood of catching a member of the target demographic in the general flow of subway traffic is undercut by the non-randomness of the canvassing approach. Street teams can profile and select whom to choose based on whether they look like they're tourists or commuters or women, etc.

Changing parameters:

If people are actually more willing to walk further than I am, we could try using a 0.5 mile or 1.0 mile radius around universities to select stations. We could use geographic data to refine the shape of the catchment area - incorporating the position of nearby stations or relative to location to major university entrances/exits, for example. 

Likewise, the boundaries of NYC tech hubs are subject to interpretation. I've seen anything from "around the Flatiron building" to Central Park/23/Broadway to Broadway/5th/Union Sq/MSQ. It might be worthwile to test different definitions to maximize capturing techies. One way I thought about testing this was to find inflection points in real estate prices (although I assume the distribution is pretty continuous in central Manhattan and the like). Several articles mention the "spread" of tech hubs out to areas (e.g. higher up, closer to GCT from the Flatiron District) that have cheaper rent. This pattern might be more visible in areas like Brooklyn or where gentrification is more obvious.

Additional variables:

We can also think about capturing populations in different ways. As mentioned, we might want to think about potential donation capture in terms of money flow rather than high-income rider flow. Additionally, we might not actually be interested in being close to tech _companies_ but rather tech _employees_. In a place like We Work (from where I write this!), there might be 15 startups registered, but only 75 people associated with them. Compare this to a single employer like Google, which employs ~2k people at their 9th Ave campus. This data would be relatively easy to find and combine.

Temporal data should also be included: though it isn't an issue for a April-May campaign, there might not be much advantage to targeting university stations in summer months, when students are likely to be away. Likewise, we might choose street team locations/times near particular topical events.

On station grouping/overlap:

With respect to geographic clustering, for example with the tech hub stations, it's particularly important to do this kind of traffic analysis because there are clear differences between stations even though they serve the same area and population. It's easy to say, well, somewhere around Flatiron, but better to know exactly which station and when.

Expanding from the subway focus, given the overlapping catchment areas of certain groups, particularly high-income areas, we might use the geographic data we generated to position teams _between_ rather than at each station to capture both populations with a single team.

However, I actually don't think it's a problem to post teams in geographically clustered stations. Because our data is (mostly) in the format of day-hour counts, we know there isn't overlap (unless there is a significant proportion of riders who use one station in the morning and nearby station at night) because a person entering at 4PM at Chambers St is not entering at 4PM at Franklin St.


Additionally, saturation gives an exposure advantage (in general, we should also consider the effects of previous campaigns and the fact that each campaign has daily cumulative effects -- should teams be sent to different stations each day, or to establish a constant presence at one station for an entire week?).

Other real-world stuff:

- Lots of high-income people in NYC every day do not actually live here but commute to outer areas like CT, in which case choosing high-volume commuter stations with rail linkages (aka GCT, Penn Station) at commute times may have an additional benefit of grabbing these types of people.

- We split days on time intervals that are likely to capture commuters/residents, but implementing this is not necessarily the best strategy because of rider behavior. Sure, lots of folks going into the subway at 23rd St at 6PM on Tuesday are tech workers, but they are also tired people who just want to get home. (This argument also applies to people exiting the subway in the morning, and actually at any time anywhere, given that subway usage implies somewhere to be.) One possibility is to use AM/PM rates to find stations with high commuter/resident populations, but to deploy around lunchtime (although no guarantees that hungry people are more sympathetic to women-in-tech issues).

Robustness:

I thought that we might be able to differentiate types of stations by traffic patterns. Stations that exhibit high weekday over weekend traffic, for example, should be commuter stations rather than tourist/residential/student stations. At least in Manhattan, however, all stations display this characteristic, presumably because workplaces are everywhere. Any uniqueness of station traffic patterns, for example due to high student proportion, is lost. 

For the data for which we do not have time-of-day breakdowns, it would also be interesting to see how station rankings compare when measured on average total daily traffic versus average traffic for a particular day-time interval dyad. This is essentially also a way of checking whether commuter patterns have visible time effects and whether we are actually capturing anything with that cut. If not, we essentially have to rely on theoretical justifications for looking for particular populations at particular times in particular stations. 

