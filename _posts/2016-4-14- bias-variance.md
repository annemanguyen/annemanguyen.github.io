---
layout: post
title: "Bias v. variance in theoretical analysis"
date: 2016-04-22 00:00:00
categories: [blog]
tags: Metis, thoughts
excerpt: "Or, how I stopped worrying and learned to love prediction . . . maybe . . ."
---

The last couple of weeks I've been having a bit of a crisis of faith. 

For the most part, my background as a former political scientist with some graduate training in statistics has been extremely useful. To some extent, though, the most interesting (and probably most important) thing that I'm trying to learn -- how to transition from sort of academic data use and manipulation to practical data science -- is also the most challenging. (Philosophically, anyway. For 'most mentally challenging' please see 'what on earth is Bayesian time series' and 'why do I keep forgetting to rename just that one variable every time'.)

Let's back up for a second. One of the main reasons I left academia is that I became less and less convinced that the current methods in use are ill-suited to capturing anything about the world. There's a push toward more math-y, science-y 'hardening' of the field (in the sense of moving away from being a 'soft' social science) that is unfortunately at odds with studying the type of phenomena that make the field sort of substantively interesting. Real-world events, particularly the ones we would love to explain, such as war, regime change, state formation, etc., are rare, self-selecting events that we can only observe, and they are messy processes for which the inputs are often hard to identify and measure.

The dream underlying political science as a discipline is a seductive one, I think. In college, I had lofty ideals of discovering *just one small true thing* about the world, of adding to the cumulative store of human knowledge. I'm now much less hubristic and much more skeptical of that notion, but I think I would still be sympathetic to the search if the process hadn't become nearly pathological due to the peculiarities of academia. 

<small>Related: it is with a certain amount of chagrin that I often find insights from political science (oten lauded as 'surprising and yet not counter-intuitive' and having required a very large and sophisticated amount of work to uncover) casually mentioned as a widely understood insight or conventional wisdom in novels and newspaper articles.</small>

---

> "(a) we don’t know enough at this point to do this exactly right and (b) in the future, what it’s taking me seven years to do with a mathematical model is going to take a computer hours, or seconds.” -- [Irene Pepperberg](http://www.newyorker.com/magazine/2008/05/12/birdbrain)

Just read this tonight while looking up Alex videos and was struck by how I feel the same way about a completely different field (Dr. Pepperberg was in theoretical chemistry). In particular, I think about all of those papers and books published in the 1990s that have simple regression models rife with endogeneity and that are basically worthless today. To that extent, I think qualitative stuff can hold more longterm value, even if it turns out to be "wrong."

---

In my Metis interview with Laurie Skelly (a very cool lady), we commiserated about academic research essentially being pushed into impracticality by the stringent requirements of making causal claims. If I want to *explain* certain phenomena, e.g. the effect of in-unit laundry on the average rental price of a 3BR in NYC (a question in which I currently have a more than academic interest), I need a model that is fully specified. It must take into account all of the factors that affect rental price (including the way those factors may affect each other). This matters when you want to actually measure the effect of any given potential explanator rather than simply to claim that it is likely that there *is* some effect. We can pretty easily show with convincing evidence that including a washer-dryer is correlated with higher rental prices, but unless we account for other factors, we don't know how much of that increase is actually due to the premium people like me place on not having to go to the laundromat, and how much is due to the fact that fancier or more recently renovated apartments that are more expensive to begin with are also more likely to have such amenities. Say I forget that elevator buildings are also more likely to have laundry rooms in the building (relaxing the in-unit constraint because, as I've found from recent empirical research, this is not actually that common). Then if I make a statistical claim, using `rental price = laundry + sq ft + neighborhood` and I find that, on average, for units with the same square footage in the same neighborhood, having a laundry room increases the price by $250/month, that estimate is actually also incorporating the effect of the building type. The true effect of laundry might be a mere $50/month, while elevator/doorman/etc. advantages account for the the other $200 difference. In general, eliding some regression assumptions, the better the entire model fits as a whole in terms of explaining variation in apartment rental prices, the more confident you can be in precise estimate of the effect of a given variable.

However, building the model in the first place is ultimately a theoretical argument. I'm guessing that apartment prices are influenced by location, size, and laundry (laundry is really important, okay?!) based on my own substantive knowledge and assumptions about real estate and what people care about when renting property. As a result, any quantitative results can be criticized basically on common sense terms even if results are statistically promising.

--- 

This is how I was trained, and I do think it's a valuable way to begin any analysis. Our most recent project was about movie data (please look forward to erading a LOT about this in upcoming posts) and it was the first time I had ever attempted any kind of regression or data analysis on a topic for which I had very little substantive knowledge. I didn't have any (and still only have a very vague) idea about how movies are made and how people choose to watch them, which made it difficult to approach a research question without being overwhelmed by caveats, data problems, etc. (Of course, from my time in political science, sometimes the more you know about something, the more it seems impossible to fully capture quantitatively...)

But that's how the real world works, right? In most non-academic settings you don't get to spend years or your whole life working on just one thing. I think there's something freeing about focusing entirely on practical problem-solving in the relative short term, and to be able to deliver a product that just works without dithering about *why* it works and whether it works on a theoretical level even if it seems to on an empirical level.

Predictive models, for example, are largely agnostic as to the included variables. I've never actually worked with biased methods such as LASSO or Ridge, but essentially they can help you pick through a huge set of variables to find the ones that combine to create the best (most error minimizing) model. Since these choices aren't guided by theory, they don't have to tell a story and they don't have to even make sense, as long as they perform well. Using a predictive approach, if my rental price equation includes some seemingly unrelated factor, such as the number of Quaker parrots in Green-Wood Cemetary that month <small> (also a factor of more-than-academic interest to me) </small>, but performs really well, we wouldn't care that it doesn't seem to make theoretical sense. (I want to note, though, that even in the explanatory approach, there might be ways to justify the inclusion of parrots on the model: maybe number of Quakers is actually a proxy for construction activity in the city, which might affect the supply/demand of apartments...) Within the equation, however, we would not be able to say anything about the specific estimates of how much a change in one of the variables affects the outcome. We *could*, however, probably say something about the direction of the change (e.g. more parrots means cheaper apartments! yay!) and maybe the magnitude.

This is a really foreign way of thinking to me, and in a way, it feels a bit like moving too far in the other direction. Even the process of data collection, of decided what set of 1000 variables to stick in your LASSO, is ultimately girded by some set of theoretical assumptions -- unless it's purely driven, I guess, by what's available. In which case, at the extreme (and one can imagine a dystopian future of data science in which this is the case), we would simply collect data on everything and use all of it every time to predict the future. The map becomes the territory and every output is input for some other process. <small>(Which is not to say that I necessarily disagree with the idea that everything can tell you about everything else, but interconnectedness as a philosophy is a different proposition than a kitchen-sink approach to regression for sheer expediency.)</small>

That's not actually going to work, of course, and part of the reason it's a bad idea even if you get a model that performs extraordinarily well is that it's less likely to have validity for out-of-sample application. We can reasonably imagine that rental prices for comparable apartments will be a good predictor for a particular unit's price into the future and around the country, etc. Some seemingly orthogonal factor that nevertheless holds predictive power for the train/test set is less likely to be generalizable. 

---

I'm open to the idea that models don't have to be perfectly specified to yield 'actionable', useful results, but I'm also skeptical of pure methods-based approaches to minimizing error. There's been some discussion in class of problems or challenges where you're given a outcome and a set of 20 or so independent variables without even know what they are. I see that it could be useful not to pollute the pure data with your ~human ~~intiutions~~ biases~ and that it could easily be the case that blind analysis yields surprising/interesting results, but I still cringe a little social science cringe.

I guess I'm still looking for a middle way. It's odd to realize that I often have the same complaint for projects on both ends of the spectrum -- *you can't use this model!* 

(I'm also still looking for an apartment. I think we gave up on laundry though.)
