#### Results

So in the previous post, I showed how I processed and analyzed the MTA data to create recommendations for the organization to deploy street teams to stations near universities. There are still several other demographics, as outlined in the general strategy! (These results were generated by other members of this project team.)

##### Total Volume

If we want to maximize exposure, we're interested in traffic as a whole, without filtering for demographics. To this end, we ranked all stations by average hourly traffic (again, between mid-April and the end of May).

![averagehourly](assets/allaveragehourly.jpg)

Usurprisingly, the top stations are major commuter hubs: GCT, 34th St, Times Square, Penn Station, and Union Square. 

I think this data would have been better presented as daily totals. It's the same number, but I think using hourly average kind of masks the fact that there are strong time-of-day trends. This doesn't really matter as far as the interpretation goes, because we're interested in all traffic at all times, but an hourly average suggests a constancy of flow that isn't really the case. 

Daily totals would also be a better match for the other pieces of data analysis, which use station-day as the unit of analysis. More temporal splitting would also allow us to find the highest traffic periods within the day, which would further help specify best street team deployments. This would require a lot more cleaning than we were able to do, though. I was lucky in that my set of university stations didn't have the time-coding issues I mentioned above, but there is significant inconsistency both among and within the whole set of stations.

##### Tech Findings

In the last post, I described how I identified the set of stations that might see usage by a higher proportion of people who work in the tech sector. Here's a map of the areas and a ranked list of stations falling within them:

![tech](assets/tech.jpg)

In general, there's a lot more traffic in Manhattan, as expected. 

This data is given as average daily total, which I think should be further restricted to weekday traffic (though I don't think the overall ranking or magnitudes of usage volume would change). Of course, we can do the same analysis as with the universities and further choose best weekday/time combinations for each station.

##### Highest Median Income

Targeting high-income residents again requires first selecting stations based on a given criterion (in this case, location in one of the top-ten highest median income census tracts) and then comparing traffic rates between those stations. Here's the list:

![highincome](assets/highincome.jpg)

This data is presented as average AM entries and PM exits in order to capture residents. Interestingly, station-time usage figures do not always covary: having the highest usage in the morning does not imply having highest usage at night. 

This breakdown also suggests the use of an interaction variable, median income * entry/exit rate, to capture the amount of money flowing through the station. This would allow us to compare, for example, the relative benefit of choosing a somewhat lower median-income station with higher traffic over the highest-income station, which may have lower traffic (e.g. you might choose Union Square at night rather than Chambers St in the morning). 

#### Conclusions

##### Overlap

![alltopstations](assets/all.jpg)

As you can see, the top five stations in each category are quite spatially distributed. I was surprised by this finding, as I assumed that they would likely dovetail, but upon further reflection, there are socioeconomic reasons why the richest neighborhoods aren't the ones closest to high-traffic transportation hubs, etc. City planning may account for some of the distribution of universities, and it may be that now-booming tech sectors were originally placed in areas away from major stations with lower real estate costs.

![overlap](assets/overlap.jpg)

This table shows that there are a few stations that might do double-duty, but unfortunately, there's no single station that is going to hit all demographics. This is a given between the university and tech stations, as they don't have a single selection in common. As far as incorporating total traffic and grabbing high-income residents at the same time as one of the interest demographics, the Columbus Circle station looks like a good compromise, but it's hardly a clear-cut winner.

##### Recommendations

So, given these findings, what do we propose to this organization? 

Generally, for this gala, I would suggest focusing on the demographically targeted stations, since finding donors and attendees is the primary goal.

However, I don't have any specific recommendations (I know it's kind of a copout) because ultimately it depends on the resources and priorities of the organization at a given time. You can imagine them saying, for example, for this campaign, we only have three street teams available per day, and it's most important for us to find young women in tech who will become the next generation of supporters. In that case we might choose more university stations despite comparatively lower traffic. The point is that the mix is flexible. For a publicity push, the organization might instead be best served placing teams at the top high-volume stations. As the organization increases its street team availability, the set of stations can expand.

The data strategy used in this project is really more about creating a set of options - a tool - that can flexibly accomodate an organization's needs. One further step might be to define a metric that allows comparisons across stations (traffic * conversion rate * income, for example?) and then build a model that lets organizations assign a multiplier that represents its priorities. 

More simply, an organization might say, we want to devote 60% of our outreach activities to well-heeled donors, 20% to tech nerds, and 20% to visibility. Then we would simply assign the available resources to each category, moving down the rankings to capture the best station/day/time in each.

##### Implementation

This is the most intriguing and yet challenging part of the project to me, as a recovering academic. We have an effectiveness model undergirded with some assumptions and we try to maximize based on those givens. This can lead to some fairly subtle - even specious? - differences that affect ranking but may not actually be signifiant in real life.  Is there really a difference between residents living in the 232k median income area near the Chambers St station and residents of the 220k median income area near Franklin St? (These stations are one stop apart, by the way, and yet, depending on the cutoff we use, we may choose Chambers and several others before we get to Franklin.)

More seriously, the limits of implementation mean that to some extent, this model may not even matter. It could be enough to simply go on theoretical intuition without using the data! I'm reminded of something an architect once told me: she said she had learned a number of complicated, highly precise equations for calculating load-bearing capacity of a building, but in the end, to guarantee safety, the result was simply multiplied by two! Likewise, these targeting approaches cut fairly finely, but any distinctions in the data are overridden by the actual process of canvassing in real life.

Consider that each member of a street team can only talk to a certain number of people a day. The traffic volume differences between stations near NYU and the City College station may not even matter, since above a fairly low threshold, most of the traffic is extraneous - you won't be able to talk to them anyway. 

This issue is partially solved by the inclusion of the exposure goal: all things equal, it is then better to have a presence at a busy university station because even though you might collect the same 100 signatures from college women studying computer science (or whatever), you have the additional benefit of 60k more sets of eyeballs seeing your logo and being reminded of your cause every day.

##### Modelfying

The good thing is that this approach is flexible and iterative. Once we do one round of outreach, we can learn from its results and do better targeting. For example, we might compare the emails collected at university stations to non-university stations. Is there a higher percentage of .edu domains? If so, we might be more sure that we are capturing the population of interest. Extrapolating further, we can use that rate as a rough estimate of targetting success and build that into the model (e.g. choosing a tech/university station gets you a 30% increase in likelihood of capturing a member of the relevant demographics).

More generally, these success rates will inform future strategies, as we can match actual donors/participants with where they were approached. 

We don't even necessarily have to collect all this data ourselves: there's a lot of both experimental and experiential data from other nonprofits engaged in similar fundraising/awareness-building efforts that can be used to estimate conversion rates, for example. It might even be useful to simply get qualitative recommendations - perhaps the layout of one station is more conducive to waylaying commuters than others.

---

If people are actually more willing to walk further than I am, we could try using a 0.5 mile or 1.0 mile radius around universities to 



- change assumptions
- add variables
##### Caveats


fuzziness of boundaries

high income people in CT, high income people who don't use the subway

spatial clustering

time capture of commuting spread


